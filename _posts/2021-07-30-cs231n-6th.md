---
layout: post
title: '[CS231n]Lecture 6-Training Neural Networks, Part 1 - 3rd'
category: [Deep Learning]
tag: [CS231n]
---

지난 포스트와 이어지는 내용입니다. Batch Normalization, babysitting the learning process 그리고 hyperparameter optimization에 대해 다룹니다.

---

# Batch Normalization

 딥러닝의 고질적인 문제인 gradient explode/vanishing은 layer가 많아지면서 초기의 작은 변화가 많은 layer를 거치면서 누적되기 때문에 발생한다. 이는 학습 효율 저하 혹은 학습 실패를 야기하므로 반드시 해결해야 한다. 적절한 activation function과 weight initialization의 선택은 이 문제를 해결하기 위한 방법이었다. 한 가지 예로, sigmoid 활성화 함수를 사용하면 saturation이 발생하므로 ReLU라는 새로운 함수가 등장했다. 또한, 효율적인 학습을 위해 Xavier나 He 초기화를 사용하기도 했다. 하지만 layer가 일정 수 이상으로 많아지면 여전히 성공적으로 학습하지 못 하는 경우가 발생했다. Batch normalization은 앞선 방법들처럼 간접적인 방법이 아닌 학습 과정 자체를 안정화함으로써 본질적으로 문제를 해결하고자 제안되었다.


## Internal Covariance Shift

 > We define *Internal Covariate Shift* as the change in the distribution of network activations due to the change in network parameters during training. 

Batch normalization을 소개한 Ioffe & Szegedy는 학습 불안정의 원인이 internal covariance shift라고 설명한다. Internal covariance shift란 layer를 통과할 때마다 입력값의 분포가 변하는 것으로 이전 layer의 파라미터가 계속 update 되기 때문에 발생한다. 결국 학습이 반복될수록 뒤쪽 layer의 분포가 심하게 변하면서 학습이 잘 되지 않게 된다. 

<p align="center">
  <img width="600" src="/public/img/internal_covariance_shift.jpeg">
</p>

Internal covariance shift를 해결하기 위한 방법으로 whitening을 생각할 수 있다. [Whitening](https://en.wikipedia.org/wiki/Whitening_transformation)은 입력값의 feature들이 uncorrelated이고 각각 분산 1을 가지게 만드는 과정이다. 하지만 이는 계산량이 많으며, 일부 파라미터가 무시될 수 있다는 문제가 있다. 입력값의 분산이 $\Sigma$라고 했을때 분산을 identity matrix로 만들기 위해 $\Sigma^{-1/2}$의 연산이 필요한데 이것은 많은 계산량을 요구한다. 또한, 입력값을 $u$, 출력값을 $x=u+b$ ($b$는 학습된 파라미터)로 하는 layer가 있다고 했을때, 평균을 빼는 과정 $x - E(x)$를 거치면 출력값에 $b$의 영향이 제거되고, 이는 결국 학습에 악영향을 준다. 표준화까지 하면 그 영향은 더 커진다고 한다. 때때로 미분불가능한 것도 whitening의 문제 중 하나이다.


## Algorithm

Whitening의 문제를 보완하고, internal covariance shift를 해결하기 위해 논문에서는 다음의 알고리즘을 제시하였다. 

<p align="center">
  <img width="450" src="/public/img/bn_algorithm.png">
</p>

많은 경우 mini-batch SGD 방법을 사용하므로, 비효율적인 입력값 전체의 full whitening보다는 batch의 feature별 정규화를 실시한다. 여기서 평균과 분산은 전체 training set에서 계산된다.

$$ \hat{x}^{(k)} = \frac{x^{(k)} - E(x^{(k)})}{\sqrt{Var(x^{(k)})}} $$

이러한 정규화는 feature들이 [decorrelated](https://en.wikipedia.org/wiki/Decorrelation)되지 않더라도 수렴 속도를 가속화한다고 알려져 있다. 하지만, -1과 1 사이의 값을 가지게 만들기 때문에 특정 활성화 함수를 사용하면 비선형 함수를 선형 함수처럼 만들어 버릴 위험이 있다.

<p align="center">
  <img width="300" src="/public/img/sigmoid_box.png">
  [-1,1] 범위로 제한되면 선형 함수와 비슷해진다.
</p>

이를 방지하기 위해 학습해야 하는 파라미터 $\gamma$ (scale factor)와 $\beta$ (shift factor)를 도입한다. 정규화된 $\hat{x}^{(k)}$에 이들을 추가한 $y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}$를 출력값으로 활성화 함수에 건네준다. 경우에 따라 입력값을 그대로 출력값으로 내보내는 identity mapping도 가능하다. BN layer는 FC/convolutional layer 다음에, 활성화 함수 layer 전에 위치하여 별도로 평균과 분산을 바꾸는 것이 아닌 신경망 내에 존재한다는 것이 whitening과 구별되는 또다른 점이라고 할 수 있다.

BN 자체로 regularization 효과도 있어서 학습 속도를 느리게 하는 dropout를 제외할 수 있고, propagation에서 파라미터 scale의 영향을 받지 않으므로 learning rate를 크게 설정하여 속도를 빠르게 한다는 이점이 있다. Test나 convolutional layer를 사용할 경우, 다른 부분이 조금 있으니 논문을 통해 이를 확인하길 바란다.



# 참고자료
1. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
2. [Batch Normalization 설명 및 구현](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-설명-및-구현/#comments)
3. [[Part IV. CNN 핵심 요소 기술] 1. Batch Normalization [1] -라온피플 머신러닝 아카데미-](https://m.blog.naver.com/laonple/220808903260)
4. [[Deep Learning] Batch Normalization 개념 정리](https://hcnoh.github.io/2018-11-27-batch-normalization)
5. [Why does batch normalization help?](https://www.quora.com/Why-does-batch-normalization-help)
