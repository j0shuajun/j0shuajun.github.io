---
layout: post
title: '[CS231n]Lecture 6-Training Neural Networks, Part 1 - 3rd'
category: [Deep Learning]
tag: [CS231n]
---

지난 포스트와 이어지는 내용입니다. Batch Normalization, babysitting the learning process 그리고 hyperparameter optimization에 대해 다룹니다.

---

# Batch Normalization

 딥러닝의 고질적인 문제인 gradient explode/vanishing은 layer가 많아지면서 초기의 작은 변화가 많은 layer를 거치면서 누적되기 때문에 발생한다. 이는 학습 효율 저하 혹은 학습 실패를 야기하므로 반드시 해결해야 한다. 적절한 activation function과 weight initialization의 선택은 이 문제를 해결하기 위한 방법이었다. 한 가지 예로, sigmoid 활성화 함수를 사용하면 saturation이 발생하므로 ReLU라는 새로운 함수가 등장했다. 또한, 효율적인 학습을 위해 Xavier나 He 초기화를 사용하기도 했다. 하지만 layer가 일정 수 이상으로 많아지면 여전히 성공적으로 학습하지 못 하는 경우가 발생했다. Batch normalization은 앞선 방법들처럼 간접적인 방법이 아닌 학습 과정 자체를 안정화함으로써 본질적으로 문제를 해결하고자 제안되었다.


## Internal Covariance Shift

 > We define *Internal Covariate Shift* as the change in the distribution of network activations due to the change in network parameters during training. 

Batch normalization을 소개한 Ioffe & Szegedy는 학습 불안정의 원인이 internal covariance shift라고 설명한다. Internal covariance shift란 layer를 통과할 때마다 입력값의 분포가 변하는 것으로 이전 layer의 파라미터가 계속 update 되기 때문에 발생한다. 결국 학습이 반복될수록 뒤쪽 layer의 분포가 심하게 변하면서 학습이 잘 되지 않게 된다. 

<p align="center">
  <img width="600" src="/public/img/internal_covariance_shift.jpeg">
</p>

Internal covariance shift를 해결하기 위한 방법으로 whitening을 생각할 수 있다. [Whitening](https://en.wikipedia.org/wiki/Whitening_transformation)은 입력값의 feature들이 uncorrelated이고 각각 분산 1을 가지게 만드는 과정이다. 하지만 이는 계산량이 많으며, 일부 파라미터가 무시될 수 있다는 문제가 있다. 입력값의 분산이 $\Sigma$라고 했을때 분산을 identity matrix로 만들기 위해 $\Sigma^{-1/2}$의 연산이 필요한데 이것은 많은 계산량을 요구한다. 또한, 입력값을 $u$, 출력값을 $x=u+b$ ($b$는 학습된 파라미터)로 하는 layer가 있다고 했을때, 평균을 빼는 과정 $x - E(x)$를 거치면 출력값에 $b$의 영향이 제거되고, 이는 결국 학습에 악영향을 준다. 표준화까지 하면 그 영향은 더 커진다고 한다. 때때로 미분불가능한 것도 whitening의 문제 중 하나이다.


## Algorithm

Whitening의 문제를 보완하고, internal covariance shift를 해결하기 위해 논문에서는 다음의 알고리즘을 제시하였다. 

<p align="center">
  <img width="450" src="/public/img/bn_algorithm.png">
</p>

많은 경우 mini-batch SGD 방법을 사용하므로, 비효율적인 입력값 전체의 full whitening보다는 batch의 feature별 정규화를 실시한다. 여기서 평균과 분산은 전체 training set에서 계산된다.

$$ \hat{x}^{(k)} = \frac{x^{(k)} - E(x^{(k)})}{\sqrt{Var(x^{(k)})}} $$

이러한 정규화는 feature들이 [decorrelated](https://en.wikipedia.org/wiki/Decorrelation)되지 않더라도 수렴 속도를 가속화한다고 알려져 있다. 하지만, -1과 1 사이의 값을 가지게 만들기 때문에 특정 활성화 함수를 사용하면 비선형 함수를 선형 함수처럼 만들어 버릴 위험이 있다.

<p align="center">
  <img width="300" src="/public/img/sigmoid_box.png">
  [-1,1] 범위로 제한되면 선형 함수와 비슷해진다.
</p>

이를 방지하기 위해 학습해야 하는 파라미터 $\gamma$ (scale factor)와 $\beta$ (shift factor)를 도입한다. 정규화된 $\hat{x}^{(k)}$에 이들을 추가한 $y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}$를 출력값으로 활성화 함수에 건네준다. 경우에 따라 입력값을 그대로 출력값으로 내보내는 identity mapping도 가능하다. BN layer는 FC/convolutional layer 다음에, 활성화 함수 layer 전에 위치하여 별도로 평균과 분산을 바꾸는 것이 아닌 신경망 내에 존재한다는 것이 whitening과 구별되는 또다른 점이라고 할 수 있다.

BN 자체로 regularization 효과도 있어서 학습 속도를 느리게 하는 dropout를 제외할 수 있고, propagation에서 파라미터 scale의 영향을 받지 않으므로 learning rate를 크게 설정하여 속도를 빠르게 한다는 이점이 있다. Test나 convolutional layer를 사용할 경우, 다른 부분이 조금 있으니 논문을 통해 이를 확인하길 바란다.



# Babysitting the Learning Process

처음부터 많은 양의 데이터를 전부 사용하기란 생각보다 쉽지 않다. 데이터의 양이 많아지면 학습 시간이 증가할텐데, 학습 중간에 오류를 발생하면 그것을 수정하는 데에 많은 시간과 노력이 필요하다. 이러한 번거로움을 피하기 위해 본격적인 학습에 앞서 적은 양의 데이터로 모의 학습을 해보자는 것이 이번 챕터의 골자이다.

첫 번째 과정은 데이터 전처리이다. 적은 양의 데이터여도 전처리는 수행해야 한다. 이전 포스트에서 설명한 바가 있기 때문에 구체적으로 설명하지 않겠다. 한 가지만 덧붙이자면, 이미지 자료의 경우 값 자체가 의미를 지니고 있기 때문에 표준화를 하지 않는 경우가 많다. 두 번째 과정은 모델(혹은 신경망 구조)의 선택이다. 후보로 정한 여러 모델을 구체적으로 디자인하면 된다. Layer가 잘 구성되었는지 확인하기 위해 강의의 예시를 살펴보자.

<p align="center">
  <img width="300" src="/public/img/correct_loss.png">
  규제화 없이 softmax를 사용한 경우
</p>

3장에서 softmax는 $-\log{x}$(밑은 자연상수 $e$이다)를 loss로 사용하는 것을 설명한 바가 있다. 예시의 경우, 10개의 클래스가 있으므로 규제를 하지 않았을 때 $\text{loss=} -\log{1/10} = 2.3$가 나와야 정상이다. 또한, 규제화 정도를 높이면 loss가 높아져야 한다. 이러한 점검 과정을 sanity check라고 하며 유사한 표현으로 double check, reality check 등이 있다.

<p align="center">
  <img width="300" src="/public/img/increase_loss.png">
  loss가 조금 증가한 결과
</p>

이제 적은 양의 데이터로 학습을 진행하자. 데이터의 양이 적기 때문에 train accuracy가 100%인 overfitting 상태가 되어야 한다. 적절한 learning rate(학습 속도)를 찾는 것도 중요하다. 너무 작게 설정하면 학습이 너무 오래 걸리고, 너무 크게 설정하면 loss가 수렴하지 않을 수 있다. 가운데 그림과 같이 초반에는 빠르게 학습을 하고 점점 속도를 줄여가는 것이 효율적이다. 규제화 정도와 학습 속도 모두 분석가에 의해 결정되는 hyperparameter이기 때문에 지금처럼 적은 데이터를 사용해 대략적인 범위로 제한한 뒤, cross validation(교차 검증)을 통해 최종 선택하는 것이 올바르다.

<p align="center">
  <img width="300" src="/public/img/learning_rate.png">
  적절한 수준의 학습 속도로 설정해야 한다.
</p>



# Hyperparameter Optimization



# 참고자료
1. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)
2. [Batch Normalization 설명 및 구현](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-설명-및-구현/#comments)
3. [[Part IV. CNN 핵심 요소 기술] 1. Batch Normalization [1] -라온피플 머신러닝 아카데미-](https://m.blog.naver.com/laonple/220808903260)
4. [[Deep Learning] Batch Normalization 개념 정리](https://hcnoh.github.io/2018-11-27-batch-normalization)
5. [Why does batch normalization help?](https://www.quora.com/Why-does-batch-normalization-help)
