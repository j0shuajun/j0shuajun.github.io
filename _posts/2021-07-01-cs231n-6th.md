---
layout: post
title: '[CS231n]Lecture 6-Training Neural Networks, Part 1 - Activation Functions'
category: [Deep Learning]
tag: [CS231n]
---

[Lecture 6-Training Neural Networks, Part 1](https://www.youtube.com/watch?v=wEoyxE0GP2M)와 강의자료를 참고하여 개인적으로 정리한 글입니다.

---

이번 강의에서는 neural network를 학습시키는 과정과 관련 내용에 대해서 공부한다. 신경망 학습 과정은 아래와 같이 정리할 수 있다.

1. One time setup (초기 설정)
    - Activation Functions
    - Data Preprocessing
    - Weight Initialization
    - Regularization
    - Gradient Checking

2. Training dynamics (신경망 학습)
    - Babysitting the Learning Process
    - parameter Updates
    - hyperparameter Optimization

3. Evaluation (모델 평가)
    - Model ensembles

6,7강에 걸쳐 위의 내용을 살펴볼텐데, 이번 강의에서 다룰 내용은 다음과 같다.
- Activation Functions
- Data Preprocessing
- Weight Initialization
- Batch Normalization
- Babysitting the Learning Process
- Hyperparameter Optimization


# Activation Functions
Activation function(활성함수)은 입력값을 받아 출력값을 생성하고 다음 layer로 전달해주는 역할을 한다. 이렇게만 설명하면 아무 함수나 사용해도 될 것 같지만 그렇지 않다. 입력값이 모델의 예측과 관련되어 있는지 판단하고 다음 layer에서 *활성화*할지 말지 결정해야 하기 때문이다.

<p align="center">
  <img width="400" src="/public/img/act_ftn.png">
</p>

가장 먼저 생각할 수 있는 활성함수로는 binary step function과 linear activation function가 있다.

## Binary Step Functions

$$
f(x) =
\begin{cases}
0 & \text{if } x \leq 0 \\
1 & \text{if } x > 0
\end{cases}
$$

Binary step function은 perceptron(퍼셉트론)에서 사용되는 활성함수로 임계치를 기준으로 0 과 1을 출력한다. 출력값이 0과 1 뿐이기 때문에 분류 기준이 세 가지 이상인 다중 분류 문제에서는 사용할 수 없다.


## Linear Activation Functions

$$f(x) = cx, \text{ where } c \text{ is a constant.}$$

Linear activation function은 단일 직선 형태의 활성함수이다. 선형 활성함수는 여러 문제가 있다.
1. 4강에서 공부한 backpropagation이 불가능하다. 역전파 과정에서 활성함수를 미분해야 하는데 미분한 결과가 데이터와 무관한 임의의 상수 $c$이기 때문에 예측에 아무런 도움이 되지 않는다.
2. 선형 활성함수를 여러번 거치는 것은 마지막에 딱 한 번 거치는 것과 동일하다.
$$f(f( \cdots f(x))) = c^n x$$
여러 layer에서 활성함수를 거치면서 데이터에 대한 분류 혹은 예측을 수행해야 하는데 선형 활성함수를 사용하면 그럴 수가 없다.


## Non-linear Activation Functions
이러한 문제 때문에 non-linear(비선형) 활성함수를 주로 사용하고, 예시로는 sigmoid, $\tanh$, ReLU, Maxout 등이 있다. 각각의 특징을 살펴보자.


### sigmoid
<p align="center">
  <img width="400" src="/public/img/sigmoid.png">
</p>

$$\sigma(x) = \frac{1}{1+e^{-x}}, ~~~\sigma'(x) = \sigma(x) (1-\sigma(x)).$$

Binary logistic regression에서 사용되는 함수이다.

Pros
1. 미분값 연산이 쉽다.
2. 출력값이 [0,1]로 제한되기 때문에 값이 explode하지 않는다.

Cons
1. Saturated neurons kill the gradients. 입력값이 커지거나 작아지면 기울기가 0에 가까워지는 현상이 발생한다. 이것이 중첩되고 곱해지면서 gradient가 0에 수렴해 버리고, 학습 효율이 떨어진다. 뿐만 아니라 미분값의 최댓값이 1/4밖에 되지 않는다.

2. 지수함수는 computational cost가 비싸다.

3. 출력값이 zero-centered가 아니며 항상 양수의 값을 가진다. 따라서 다음 layer가 항상 양의 입력값만을 받게 되어 역전파 과정에서 무조건 양 혹은 무조건 음의 방향으로만 진행하게 된다. 이는 수렴 속도가 느려지는 결과를 야기한다.

<p align="center">
  <img width="400" src="/public/img/zigzag.png">
</p>

> 파란색 화살표는 hypothetical optimal direction이다. sigmoid 함수를 사용하면 optimal direction으로 갈 수 없고, 빨간 zigzag path처럼 느리게 업데이트된다.

이러한 단점때문에 최근에는 sigmoid 함수를 거의 사용하지 않는다.


### tanh
<p align="center">
  <img width="400" src="/public/img/tanh.png">
</p>

$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

Sigmoid 함수가 zero-centered 가 아닌 것을 보완하기 위해 제안되었다. 하지만, 그 외의 단점은 동일하기 때문에 잘 사용하지 않는다.


### ReLU
<p align="center">
  <img width="400" src="/public/img/relu.png">
</p>
가장 많이 사용되는 활성함수 중 하나이다. Rectified Linear Unit 의 약자로 선형함수의 단점을 보완한 것이다.

$$f(x) = \max(0,x)$$

Pros
1. 양수 영역에서는 saturate하지 않는다.
2. 연산이 굉장히 효율적이다.
3. Sigmoid, $\tanh$보다 수렴속도가 6배 정도 빠르다.
4. Actually more biologically plausible than sigmoid. (4강의 뉴런 이야기)

Cons
1. Not zero-centered.
2. 0에서 미분 불가능하다.
3. 입력값이 음수라면 gradient가 영영 죽어버린다.

Gradient가 0이 되어버리는 ReLU의 단점을 보완하기 위해 유사한 활성함수가 고안되었다.

#### Leaky ReLU
<p align="center">
  <img width="400" src="/public/img/leaky_relu.png">
</p>

$$f(x) = \max(0.01x, x)$$

음수 영역만 조금 다르게 한 것으로, ReLU의 모든 장점을 다 가지고 있으면서 gradient가 절대 죽지 않는다. Leaky는 새고 있다는 뜻이다. 비슷한 것으로는 Parametric Rectifier(PReLU)가 있다.

$$f(x) = \max(\alpha x, x)$$

여기서 $\alpha$는 parameter 이므로 역전파 과정에서 업데이트 된다.


#### ELU
<p align="center">
  <img width="400" src="/public/img/elu.png">
</p>

$$
f(x) =
\begin{cases}
x & \text{if } x > 0 \\
\alpha(e^x - 1) & \text{if } x \geq 0
\end{cases}
$$

Exponential Linear Unit의 약자로 ReLU의 모든 장점을 가지고 있으며 조금 더 zero-centered되어 있다. 음수 영역에서의 saturate regime이 leaky ReLU보다 noise에 robust하게 해준다. 하지만 지수함수가 있어서 연산비용이 비싸다는 단점이 있다.


### Maxout

$$f( \mathbf{x}) = \max_i x_i$$

간단하게 설명하면 입력값 중 최댓값을 출력값으로 하는 활성함수이다. Dropout의 효과를 극대화하기 위해 고안된 활성함수이다. ReLU의 장점을 모두 가지고 있다. 단점으로 비싼 연산비용이 있다.

이 외에도 수많은 [활성함수](https://en.wikipedia.org/wiki/Activation_function)가 있다. 모든 문제에 최적인 활성함수는 없으므로 데이터를 보고 알맞게 선택할 필요가 있겠다.

* Activation function 관련 참고자료
1. [Difference of Activation Functions in Neural Networks in general](https://datascience.stackexchange.com/questions/14349/difference-of-activation-functions-in-neural-networks-in-general)
2. [[라온피플 : Machine Learning Academy_Part VI. CNN 핵심 요소 기술] 4.Maxout](https://blog.naver.com/laonple/220836305907)
3. [Fundatmentals of Deep Learning - Activation Functions and When to Use Them?](https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/)
4. [A Practical Guide to ReLU](https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7)
5. [Coding Neural Network - Forward Propagation and Backpropagation](https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76)


작성 중